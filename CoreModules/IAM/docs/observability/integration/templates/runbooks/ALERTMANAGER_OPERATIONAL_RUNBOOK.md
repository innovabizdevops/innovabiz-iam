# INNOVABIZ-OPS-AM01 - AlertManager Operational Runbook

## üö® Vis√£o Geral

**M√≥dulo:** INNOVABIZ Observability Framework  
**Componente:** AlertManager  
**Vers√£o:** 0.26.0  
**Equipe Respons√°vel:** Observability Team  
**Contatos Prim√°rios:** observability@innovabiz.com, #observability-support  
**Reposit√≥rio:** `CoreModules/IAM/docs/observability`  

## üìã Informa√ß√µes Gerais

O AlertManager √© um componente cr√≠tico da pilha de observabilidade INNOVABIZ, respons√°vel pelo gerenciamento, agrupamento, roteamento e entrega de alertas gerados pelo Prometheus. Este runbook fornece orienta√ß√µes para opera√ß√µes, troubleshooting e manuten√ß√£o do AlertManager implantado nos clusters Kubernetes INNOVABIZ.

## üîê Multi-Contexto e Compliance

O AlertManager da INNOVABIZ opera em conformidade com:

- **Multi-Tenant:** Isolamento completo por tenant com roteamento espec√≠fico
- **Multi-Regional:** Configura√ß√£o adaptada por regi√£o (BR, US, EU, AO)
- **Multi-Ambiente:** Separa√ß√£o entre produ√ß√£o, homologa√ß√£o e desenvolvimento
- **Multi-Moeda:** Contextos financeiros por moeda (BRL, USD, EUR, AOA)
- **Multi-Idioma:** Templates de notifica√ß√£o localizados (pt-BR, pt-AO, en-US, en-GB)
- **Compliance:** PCI DSS 4.0, GDPR/LGPD, ISO 27001, NIST CSF, SOX, Basel III

## üìä Arquitetura e Componentes

### Componentes Principais

- **AlertManager UI:** Interface web para visualiza√ß√£o e gerenciamento de alertas
- **Cluster Mesh:** Sincroniza√ß√£o entre m√∫ltiplas inst√¢ncias do AlertManager
- **Silence API:** API para cria√ß√£o e gerenciamento de silenciamentos
- **Notification Pipeline:** Sistema de processamento e entrega de notifica√ß√µes
- **Canais Configurados:** Email, Slack, PagerDuty, OpsGenie, Webhooks

### Interdepend√™ncias

1. **Prometheus:** Origem prim√°ria dos alertas
2. **Grafana:** Visualiza√ß√£o alternativa dos alertas ativos
3. **KrakenD:** API Gateway para exposi√ß√£o segura da API
4. **IAM Service:** Autentica√ß√£o e autoriza√ß√£o
5. **Redis:** Cache para estado compartilhado (opcional)
6. **Servi√ßos SMTP:** Para notifica√ß√µes por email

## üîç Procedimentos Operacionais

### üü¢ Verifica√ß√£o de Status

```bash
# Verificar estado dos pods do AlertManager
kubectl get pods -n observability-${TENANT_ID}-${REGION} -l app=alertmanager

# Verificar logs em tempo real
kubectl logs -f -n observability-${TENANT_ID}-${REGION} -l app=alertmanager

# Acessar a interface web (via port-forward para testes)
kubectl port-forward -n observability-${TENANT_ID}-${REGION} svc/alertmanager 9093:9093
# Acessar: http://localhost:9093
```

### üü¢ Verifica√ß√µes de Sa√∫de

```bash
# Status do componente
curl -sSL http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/-/healthy

# Verificar m√©tricas expostas
curl -sSL http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/metrics | grep alertmanager

# Status do cluster (informa√ß√µes sobre membros do cluster mesh)
curl -sSL http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/-/ready
```

### üîÑ Manuten√ß√£o de Rotina

#### Atualiza√ß√µes de Vers√£o

1. Consulte o [Changelog oficial](https://github.com/prometheus/alertmanager/releases) para impactos
2. Atualize a vers√£o no manifesto YAML ou Helm Chart
3. Realize o deploy em ambientes de menor criticidade primeiro
4. Monitore m√©tricas de performance e integridade ap√≥s a atualiza√ß√£o
5. Execute smoke tests para garantir entrega de notifica√ß√µes

```bash
# Atualizar a vers√£o via kubectl
kubectl -n observability-${TENANT_ID}-${REGION} set image deployment/alertmanager \
  alertmanager=quay.io/prometheus/alertmanager:v0.26.0
```

#### Backups de Configura√ß√£o

```bash
# Backup das configura√ß√µes e silenciamentos
kubectl -n observability-${TENANT_ID}-${REGION} get configmap alertmanager-config -o yaml > alertmanager-config-backup-$(date +%Y%m%d).yaml

# Backup do persistent volume (se aplic√°vel)
kubectl -n observability-${TENANT_ID}-${REGION} get pvc alertmanager-storage -o yaml > alertmanager-pvc-backup-$(date +%Y%m%d).yaml
```

## üîé Troubleshooting

### Problema: Alertas N√£o Est√£o Sendo Enviados

#### Diagn√≥stico

1. Verificar se os alertas est√£o chegando ao AlertManager:
   ```bash
   curl -s http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/api/v1/alerts | jq
   ```

2. Verificar logs para erros de entrega:
   ```bash
   kubectl logs -f -n observability-${TENANT_ID}-${REGION} -l app=alertmanager | grep -E "error|failed"
   ```

3. Verificar a configura√ß√£o de roteamento:
   ```bash
   kubectl -n observability-${TENANT_ID}-${REGION} get configmap alertmanager-config -o yaml
   ```

#### Resolu√ß√£o

1. **Problemas de conectividade SMTP:**
   - Verificar credenciais SMTP no secret
   - Testar conectividade ao servidor SMTP
   - Revisar logs para erros espec√≠ficos de SMTP

2. **Problemas com Slack/API externa:**
   - Validar token/URL no secret
   - Verificar conectividade com a API externa
   - Revisar pol√≠ticas de rede para egress

3. **Problemas de roteamento:**
   - Revisar labels usados nos alertas
   - Verificar regras de match/mismatch
   - Verificar se receivers est√£o corretamente configurados

### Problema: Alta Carga de Alertas ou Performance Degradada

#### Diagn√≥stico

1. Verificar m√©tricas de performance:
   ```bash
   curl -s http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/metrics | grep -E "(alertmanager_alerts|alertmanager_notifications|process_cpu|process_resident)"
   ```

2. Analisar n√∫mero de alertas por tenant/regi√£o:
   ```bash
   curl -s http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/api/v1/alerts | jq '.[] | .labels.tenant_id' | sort | uniq -c
   ```

#### Resolu√ß√£o

1. **Alta carga de alertas:**
   - Revisar limites de agrupamento
   - Aumentar recursos do deployment
   - Escalar horizontalmente (aumentar r√©plicas)

2. **Problemas de performance:**
   - Verificar CPU/mem√≥ria e ajustar limites
   - Otimizar regras de agrupamento para reduzir processamento
   - Avaliar configura√ß√µes de throttling para receivers

### Problema: Sincroniza√ß√£o entre Inst√¢ncias do AlertManager

#### Diagn√≥stico

1. Verificar status do cluster:
   ```bash
   curl -s http://alertmanager.${TENANT_ID}.${REGION}.innovabiz.io/api/v1/status | jq '.cluster'
   ```

2. Verificar conectividade entre pods:
   ```bash
   kubectl -n observability-${TENANT_ID}-${REGION} exec -it alertmanager-0 -- wget -qO- alertmanager-1.alertmanager:9094/metrics
   ```

#### Resolu√ß√£o

1. **Problemas de sincroniza√ß√£o:**
   - Verificar NetworkPolicy permitindo tr√°fego entre pods
   - Garantir que ports 9094 est√£o abertos para cluster mesh
   - Validar que todos os pods t√™m a mesma configura√ß√£o de cluster

## üîí Seguran√ßa e Compliance

### Controles de Seguran√ßa Implementados

- TLS para toda comunica√ß√£o externa (via Ingress)
- Autentica√ß√£o b√°sica na interface web
- Execu√ß√£o como usu√°rio n√£o-root (65534)
- Filesystem read-only
- NetworkPolicy restritiva
- Secrets para credenciais sens√≠veis
- Audit logging ativado
- Headers de seguran√ßa HTTP (Strict-Transport-Security, X-Content-Type-Options)
- Preven√ß√£o de CSRF e XSS

### Verifica√ß√µes de Compliance

```bash
# Verificar permiss√µes RBAC
kubectl -n observability-${TENANT_ID}-${REGION} get role alertmanager-role -o yaml

# Verificar SecurityContext
kubectl -n observability-${TENANT_ID}-${REGION} get pods -l app=alertmanager -o jsonpath='{.items[0].spec.containers[0].securityContext}'

# Verificar configura√ß√µes de NetworkPolicy
kubectl -n observability-${TENANT_ID}-${REGION} get networkpolicy alertmanager-network-policy -o yaml
```

## üìà Monitoramento e M√©tricas

### M√©tricas Cr√≠ticas

| M√©trica | Descri√ß√£o | Thresholds |
|---------|-----------|------------|
| `alertmanager_alerts` | N√∫mero total de alertas | Alerta: >1000 |
| `alertmanager_alerts_received_total` | Total de alertas recebidos | Tend√™ncia: >30% do normal |
| `alertmanager_notifications_failed_total` | Notifica√ß√µes com falha | Alerta: >5% |
| `alertmanager_notification_latency_seconds` | Lat√™ncia de notifica√ß√£o | Alerta: >10s |
| `alertmanager_cluster_members` | Membros do cluster | Alerta: <configurado |
| `process_cpu_seconds_total` | Utiliza√ß√£o de CPU | Alerta: >80% |
| `process_resident_memory_bytes` | Utiliza√ß√£o de mem√≥ria | Alerta: >80% |
| `alertmanager_silences` | N√∫mero de silenciamentos ativos | Info: monitorar tend√™ncia |
| `alertmanager_nflog_gc_duration_seconds` | Tempo de garbage collection | Alerta: >5s |

### Dashboards Recomendados

- **AlertManager Overview:** Vis√£o geral de sa√∫de e performance
- **AlertManager por Tenant:** M√©tricas separadas por tenant
- **AlertManager por Regi√£o:** M√©tricas separadas por regi√£o
- **Alert Delivery SLOs:** M√©tricas de SLO para entrega de alertas
- **Alertas Multi-Dimensionais:** Visualiza√ß√£o por tenant/regi√£o/ambiente
- **Alert Heatmap:** Distribui√ß√£o temporal de alertas

## üìö Melhores Pr√°ticas

### Design de Alertas

- **Severidade Consistente:** Usar labels `severity` padronizados (critical, warning, info)
- **Contexto Multi-dimensional:** Incluir sempre `tenant_id`, `region_id`, `environment`, `component`
- **Acionabilidade:** Cada alerta deve ter uma a√ß√£o clara a ser tomada
- **Documenta√ß√£o:** Incluir links para runbooks ou documenta√ß√£o em `annotations.runbook_url`
- **Correla√ß√£o:** Usar labels que permitam correlacionar alertas relacionados
- **Precis√£o:** Evitar falsos positivos com thresholds bem calibrados
- **Prioriza√ß√£o:** Usar `priority` label para indicar urg√™ncia (P1-P5)

### Agrupamento Eficiente

- Agrupar por `tenant_id`, `severity`, `alertname` e `region_id`
- Estabelecer intervalos de grupo adequados por severidade
- Evitar "alert storms" com regras de inibi√ß√£o apropriadas
- Utilizar sil√™ncios com data de expira√ß√£o e justificativa
- Implementar routes espec√≠ficas para diferentes tipos de alerta
- Configurar timeouts e repeti√ß√µes adequadas por severidade
- Utilizar continue: true para encaminhamento multi-canal quando necess√°rio

### Notifica√ß√µes Efetivas

- Personalizar templates por canal de notifica√ß√£o
- Incluir links para Grafana e sistemas de ticketing
- Formata√ß√£o clara com indicadores visuais de severidade
- Enviar para canais apropriados com base no hor√°rio e escalation path
- Incluir m√©tricas relevantes e contexto do problema
- Localizar mensagens conforme idioma regional (pt-BR, pt-AO, en-US, etc)
- Adicionar informa√ß√µes de SLA baseadas em severidade e tenant

## üìÜ Manuten√ß√£o e Ciclo de Vida

### Verifica√ß√µes Peri√≥dicas

| Frequ√™ncia | Atividade | Respons√°vel |
|------------|-----------|-------------|
| Di√°rio | Validar recebimento e processamento de alertas | SRE On-call |
| Semanal | Revisar m√©tricas de desempenho | DevOps |
| Semanal | Revisar silenciamentos expirados | DevOps |
| Mensal | Testar recupera√ß√£o de backup | SRE Team |
| Mensal | Revisar alertas mais frequentes para redu√ß√£o de ru√≠do | Observability Team |
| Trimestral | Revisar configura√ß√µes e rotas | Observability Team |
| Trimestral | Teste de carga do sistema de alertas | Performance Team |
| Anual | Avaliar atualiza√ß√µes de vers√£o | Platform Team |
| Anual | Auditoria completa de seguran√ßa e compliance | Security Team |

### Melhorias Cont√≠nuas

- Revisar alertas mais frequentes para redu√ß√£o de ru√≠do
- Otimizar templates de notifica√ß√£o para maior clareza
- Refinar regras de agrupamento para melhor organiza√ß√£o
- Calibrar thresholds com base em dados hist√≥ricos
- Analisar tempos de resposta e resolu√ß√£o para aprimoramento
- Implementar automa√ß√µes para alertas repetitivos
- Desenvolver machine learning para detec√ß√£o de anomalias
- Integrar com sistemas de an√°lise p√≥s-incidente

## üîó Integra√ß√£o com Outros Componentes

### Prometheus

```yaml
# Exemplo de configura√ß√£o no Prometheus
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.${TENANT_ID}.${REGION}.innovabiz.io
    api_version: v2
    timeout: 10s
    scheme: https
    tls_config:
      ca_file: /etc/prometheus/certs/ca.crt
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_tenant_id]
        target_label: tenant_id
      - source_labels: [__meta_kubernetes_pod_label_region_id]
        target_label: region_id
```

### Grafana

- Configurar AlertManager como data source em Grafana
- Utilizar integra√ß√£o para Unified Alerting quando aplic√°vel
- Configurar dashboards espec√≠ficos para visualiza√ß√£o de alertas
- Implementar anota√ß√µes em gr√°ficos para visualizar eventos de alerta

### OpsGenie/PagerDuty

- Configurar integra√ß√£o bidirecional para sincroniza√ß√£o de status
- Mapeamento de severidades entre sistemas
- Escalation policies alinhadas √† estrutura organizacional
- Tags e atributos para preservar contexto multi-dimensional

### ITSM/ServiceNow

- Cria√ß√£o autom√°tica de tickets baseada em alertas
- Campos personalizados para contexto multi-dimensional
- Regras de prioridade e categoriza√ß√£o alinhadas
- Fechamento autom√°tico baseado em resolu√ß√£o de alerta

## üìù Documenta√ß√£o Relacionada

- [INNOVABIZ Alerting Framework](../ALERTING_FRAMEWORK.md)
- [INNOVABIZ Prometheus Template](./KUBERNETES_YAML/PROMETHEUS_TEMPLATE.md)
- [INNOVABIZ AlertManager Template](./KUBERNETES_YAML/ALERTMANAGER_TEMPLATE.md)
- [INNOVABIZ Observability Policy](../../../policies/OBSERVABILITY_POLICY.md)
- [Documenta√ß√£o Oficial AlertManager](https://prometheus.io/docs/alerting/latest/alertmanager/)

## üìû Suporte e Escalonamento

| N√≠vel | Contato | SLA |
|-------|---------|-----|
| L1 | SRE On-call: sre-oncall@innovabiz.com | 15min |
| L2 | DevOps Team: devops@innovabiz.com | 30min |
| L3 | Observability Team: observability@innovabiz.com | 60min |

**Em caso de emerg√™ncia:** +55 11 95555-1234

**Canais de comunica√ß√£o:**
- Slack: #observability-support, #sre-alerts
- Email: observability@innovabiz.com
- Teams: Observability Team
- Hor√°rio comercial: 08:00-18:00 (todas as regi√µes suportadas)

## üìÖ Hist√≥rico de Revis√µes

| Data | Vers√£o | Autor | Descri√ß√£o |
|------|--------|-------|-----------|
| 31/07/2025 | 1.0.0 | Eduardo Jeremias | Vers√£o inicial |